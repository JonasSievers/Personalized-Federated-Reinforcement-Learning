{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "# Load, PV, Price Data Generation --------------------------------------------------------\n",
    "def generate_load_data(time_index, random_seed):\n",
    "\n",
    "    # Set the random seed if provided\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    #Baseload\n",
    "    base_load = np.random.uniform(1, 1.5)\n",
    "\n",
    "    # Daily load pattern: higher in the morning and evening\n",
    "    hour = time_index.hour\n",
    "    daily_variation = np.where((hour >= 7) & (hour <= 9), 1.2, 1.0)\n",
    "    daily_variation = np.where((hour >= 18) & (hour <= 21), 1.3, daily_variation)\n",
    "    \n",
    "    # Seasonal variation: higher consumption in winter\n",
    "    day_of_year = time_index.dayofyear\n",
    "    seasonal_variation = 1 + 0.1 * np.cos(2 * np.pi * (day_of_year - 172) / 365)\n",
    "    \n",
    "    # Random noise\n",
    "    noise = np.random.normal(0, 0.1, size=len(time_index))\n",
    "    \n",
    "    # Load calculation\n",
    "    load = base_load * daily_variation * seasonal_variation + noise\n",
    "    load = np.clip(load, 0, None)  # Ensure load is non-negative\n",
    "    \n",
    "    return pd.Series(load, index=time_index)\n",
    "\n",
    "def generate_price_data(time_index, random_seed):\n",
    "    \"\"\"\n",
    "    Generate realistic dynamic electricity prices for Germany in €/kWh.\n",
    "\n",
    "    Parameters:\n",
    "    - time_index (pd.DatetimeIndex): The datetime index for which to generate prices.\n",
    "    - random_seed (int, optional): Seed for random number generator for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: Electricity prices in €/kWh indexed by the provided time_index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize random seed\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "\n",
    "    # Base price in €/kWh\n",
    "    base_price = 0.03  # Adjusted baseline for visual clarity\n",
    "\n",
    "    # Daily pattern: two distinct peaks (morning and evening)\n",
    "    hours = time_index.hour + time_index.minute / 60.0  # Fractional hours for smooth patterns\n",
    "\n",
    "    # Morning peak (higher)\n",
    "    morning_peak = 0.008 * np.exp(-0.5 * ((hours - 8) / 2) ** 2)  # Centered at 8 AM, narrow\n",
    "\n",
    "    # Evening peak (highest)\n",
    "    evening_peak = 0.012 * np.exp(-0.5 * ((hours - 19) / 2) ** 2)  # Centered at 7 PM, narrow\n",
    "\n",
    "    # Combine morning and evening peaks\n",
    "    daily_pattern = morning_peak + evening_peak\n",
    "\n",
    "    # Weekly pattern: slightly lower prices on weekends\n",
    "    day_of_week = time_index.dayofweek  # Monday=0, Sunday=6\n",
    "    weekend = (day_of_week >= 5).astype(float)  # 1 for Saturday/Sunday, 0 otherwise\n",
    "    weekly_adjustment = -0.005 * weekend  # Reduce prices on weekends\n",
    "\n",
    "    # Seasonal trend: higher in winter and summer, lower in spring/autumn\n",
    "    day_of_year = time_index.dayofyear\n",
    "    seasonal_amplitude = 0.005  # Reduced amplitude for clarity\n",
    "    seasonal_pattern = seasonal_amplitude * np.sin((day_of_year - 80) / 365 * 2 * np.pi)\n",
    "\n",
    "    # Random noise\n",
    "    noise_std = 0.002  # Reduced noise for clearer patterns\n",
    "    noise = rng.normal(0, noise_std, size=len(time_index))\n",
    "\n",
    "    # Combine all components\n",
    "    prices = base_price + daily_pattern + weekly_adjustment + seasonal_pattern + noise\n",
    "\n",
    "    # Ensure prices are within a realistic range (e.g., €0.02/kWh to €0.10/kWh)\n",
    "    #prices = np.clip(prices, 0.02, 0.10)\n",
    "\n",
    "    # Create a pandas Series\n",
    "    price_series = pd.Series(prices, index=time_index)\n",
    "    price_series.name = 'Electricity Price (€ / kWh)'\n",
    "\n",
    "    return price_series\n",
    "\n",
    "def generate_pv_data(time_index, random_seed):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Create an array to hold the PV generation values\n",
    "    pv_generation = np.zeros(len(time_index))\n",
    "    \n",
    "    # Define parameters for the bell curve\n",
    "    # Random PV capacity between 2 and 5 kW\n",
    "    peak_capacity = np.random.uniform(2, 5) # Maximum capacity of the PV panel (in kW)\n",
    "    peak_time = 12      # Time of peak generation (12 PM)\n",
    "    std_dev = 2         # Standard deviation for the bell curve\n",
    "\n",
    "    # Generate synthetic PV data following a bell curve\n",
    "    for i, t in enumerate(time_index):\n",
    "        if 6 <= t.hour < 19:  # Daytime hours (6 AM to 6 PM)\n",
    "            # Calculate the hour of the day (0-23)\n",
    "            hour = t.hour + t.minute / 60\n",
    "            \n",
    "            # Calculate Gaussian distribution value\n",
    "            gaussian_value = np.exp(-((hour - peak_time) ** 2) / (2 * std_dev ** 2))\n",
    "            \n",
    "            # Scale it to the peak capacity\n",
    "            pv_generation[i] = peak_capacity * gaussian_value\n",
    "            \n",
    "            # Add random variation to simulate cloud cover\n",
    "            random_variation = np.random.normal(0, 0.9)  # Random noise with mean=0 and std=10\n",
    "            pv_generation[i] += random_variation\n",
    "            \n",
    "            # Ensure generation is non-negative\n",
    "            pv_generation[i] = max(0, pv_generation[i])\n",
    "        else:\n",
    "            pv_generation[i] = 0  # Nighttime hours\n",
    "\n",
    "    return pd.Series(pv_generation, index=time_index)\n",
    "\n",
    "def generate_ems_data(ems_id, start_date, end_date, cong_rebate, cong_weekdays, cong_hours, random_seed):\n",
    "    # Generate time index\n",
    "    time_index = pd.date_range(start=start_date, end=end_date, freq='h')\n",
    "    \n",
    "    # Generate common price data\n",
    "    price_data = generate_price_data(time_index, random_seed)\n",
    "        \n",
    "    # Generate load data\n",
    "    load_data = generate_load_data(time_index, random_seed)\n",
    "        \n",
    "    # Generate PV data\n",
    "    pv_data = generate_pv_data(time_index, random_seed)\n",
    "        \n",
    "    # Combine data into a DataFrame\n",
    "    ems_df = pd.DataFrame({\n",
    "        'Time': time_index,\n",
    "        'EMS_ID': ems_id,\n",
    "        'Load': load_data.values,\n",
    "        'PV_Generation': pv_data.values,\n",
    "        'Price': price_data.values,\n",
    "    })\n",
    "    ems_df['Congestion'] = ((ems_df['Time'].dt.weekday.isin(cong_weekdays)) & (ems_df['Time'].dt.hour.isin(cong_hours))).astype(int)\n",
    "    ems_df['Valid_Baseline_Day'] = (~ems_df.groupby(ems_df['Time'].dt.date)['Congestion'].transform('any')).astype(int)\n",
    "    ems_df['FlexPrice'] = ems_df['Congestion'] * cong_rebate\n",
    "    \n",
    "    return ems_df\n",
    "\n",
    "def plot_synthetic_data_v1(data, ems_id, start_date, end_date):\n",
    "\n",
    "    mask = (data['EMS_ID'] == ems_id) & (data['Time'] >= start_date) & (data['Time'] < end_date)\n",
    "\n",
    "    plt.figure(figsize=(15, 3))\n",
    "\n",
    "    # Create the first y-axis\n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(data.loc[mask, 'Time'], data.loc[mask, 'Load'], label='Load')\n",
    "    ax1.plot(data.loc[mask, 'Time'], data.loc[mask, 'PV_Generation'], label='PV Generation')\n",
    "\n",
    "    # Create the second y-axis for Price\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(data.loc[mask, 'Time'], data.loc[mask, 'Price'], color=\"green\", label='Price')\n",
    "\n",
    "    # Set titles and labels\n",
    "    ax1.set_title(f'EMS_ID {ems_id} Load, PV Generation, and Price')\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Power (kW)')\n",
    "    ax2.set_ylabel('Price ($/kWh)')\n",
    "\n",
    "    # Combine legends from both axes\n",
    "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Optimization Problem for BESS Data Generation ------------------------------------------\n",
    "def precompute_baseline_indices(data, baseline_lookback, max_lookback):\n",
    "\n",
    "    n_hours_per_day = 24  # Assuming hourly data\n",
    "\n",
    "    # Extract 'Valid_Baseline_Day' as a NumPy array for faster access\n",
    "    valid_baseline = data['Valid_Baseline_Day'].to_numpy()\n",
    "\n",
    "    # Get the actual indices of the DataFrame\n",
    "    actual_indices = data.index.to_list()\n",
    "\n",
    "    # Initialize the result dictionary\n",
    "    baseline_indices = {}\n",
    "\n",
    "    # Iterate over each index in the DataFrame\n",
    "    for current_idx in actual_indices:\n",
    "        past_indices = []\n",
    "        # Iterate over each lookback day\n",
    "        for n in range(1, max_lookback + 1):\n",
    "            shifted_idx = current_idx - n * n_hours_per_day\n",
    "            if shifted_idx not in actual_indices:\n",
    "                continue  # Skip if shifted index is out of bounds\n",
    "            else:\n",
    "                # Find the position of shifted_idx in the actual_indices list\n",
    "                shifted_pos = actual_indices.index(shifted_idx)\n",
    "                if valid_baseline[shifted_pos] != 0:\n",
    "                    past_indices.append(shifted_idx)\n",
    "                    \n",
    "                if len(past_indices) >= baseline_lookback:\n",
    "                    break  # Collected enough baseline indices\n",
    "\n",
    "        # Ensure the current index is included if past_indices is empty\n",
    "        if not past_indices:\n",
    "            past_indices.append(current_idx)\n",
    "            \n",
    "        baseline_indices[current_idx] = past_indices\n",
    "\n",
    "    return baseline_indices\n",
    "\n",
    "def build_model(data, baseline_indices, bess_params):\n",
    "    \n",
    "    # Create a Gurobi model\n",
    "    model = gp.Model(\"BESS_Optimization\")\n",
    "\n",
    "    # Extract time periods\n",
    "    time_periods = data.index.to_list()\n",
    "\n",
    "    # Precompute data\n",
    "    load = data['Load'].to_numpy()\n",
    "    pv_gen = data['PV_Generation'].to_numpy()\n",
    "    price = data['Price'].to_numpy()\n",
    "    flex_price = data['FlexPrice'].to_numpy()\n",
    "        \n",
    "    # Decision Variables\n",
    "    s_power  = model.addVars(time_periods, lb=-bess_params['s_max'], ub=bess_params['s_max'], name=\"s_power\")\n",
    "    SOC = model.addVars(time_periods, lb=bess_params['soc_min'], ub=bess_params['soc_max'], name=\"SOC\")\n",
    "    net_consumption = model.addVars(time_periods, lb=-100, ub=100, name=\"net_consumption\")\n",
    "    baseline = model.addVars(time_periods, lb=-100, ub=100, name=\"baseline\")\n",
    "    F_pos = model.addVars(time_periods, lb=0, ub=100, name=\"F_pos\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Add Constraints\n",
    "    # ----------------------------\n",
    "    \n",
    "    # SOC Dynamics Constraints\n",
    "    model.addConstrs(\n",
    "    (SOC[t] == (bess_params['soc_init'] if t == time_periods[0] else SOC[time_periods[time_periods.index(t) - 1]]) + s_power[t] * bess_params['eta'] for t in time_periods),\n",
    "        name=\"SOC_dynamics\"\n",
    "    )   \n",
    "\n",
    "    model.addConstrs(\n",
    "        (net_consumption[t] == load[time_periods.index(t)] - pv_gen[time_periods.index(t)] + s_power[t] for t in time_periods),\n",
    "        name=\"NetConsumption\"\n",
    "    )\n",
    "\n",
    "    # Baseline Constraint\n",
    "    for t in time_periods:\n",
    "        model.addConstr(\n",
    "            (baseline[t] == gp.quicksum(net_consumption[p] for p in baseline_indices[t]) / len(baseline_indices[t])),\n",
    "            name=f\"Baseline\"\n",
    "        )\n",
    "\n",
    "    #Flexibility Constraint\n",
    "    M=500\n",
    "    for t in time_periods:\n",
    "        z = model.addVar(vtype=GRB.BINARY, name=f\"z_{t}\")\n",
    "        model.addConstr(F_pos[t] >= baseline[t] - net_consumption[t])\n",
    "        model.addConstr(F_pos[t] >= 0)\n",
    "        model.addConstr(F_pos[t] <= (baseline[t] - net_consumption[t]) + M * z)\n",
    "        model.addConstr(F_pos[t] <= M * (1 - z))\n",
    "   \n",
    "    # Objective function\n",
    "    total_cost = gp.quicksum(\n",
    "        price[time_periods.index(t)] * (load[time_periods.index(t)] - pv_gen[time_periods.index(t)] + s_power[t])\n",
    "        - flex_price[time_periods.index(t)] * F_pos[t]\n",
    "        for t in time_periods\n",
    "    )\n",
    "    model.setObjective(total_cost, GRB.MINIMIZE)\n",
    "\n",
    "    return model\n",
    "\n",
    "def optimize_model(model):\n",
    "    \"\"\"\n",
    "    Optimize the given Gurobi model.\n",
    "    \"\"\"\n",
    "    # Set Gurobi parameters for better performance\n",
    "    model.setParam('OutputFlag', 0)        # Suppress Gurobi output\n",
    "    model.setParam('Threads', 8)            # Adjust based on your CPU (e.g., 8 threads for an 8-core CPU)\n",
    "    model.setParam('Presolve', 2)           # Automatic presolve\n",
    "    #model.setParam('Cuts', 2)               # Automatic cutting planes\n",
    "    #model.setParam('Heuristics', 0.1)       # Allocate 10% of time to heuristics\n",
    "    #model.Params.MIPGap = 0.01  # 1% optimality gap\n",
    "    \n",
    "    # Optimize the model\n",
    "    model.optimize()\n",
    "    return model\n",
    "\n",
    "def extract_results(model, data):\n",
    "    \"\"\"\n",
    "    Extract the optimization results and add them to the data DataFrame.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store variable values\n",
    "    s_power_values = []\n",
    "    SOC_values = []\n",
    "    Baseline_values = []\n",
    "    F_pos_values = []\n",
    "    NetConsumption_Optimized = []\n",
    "\n",
    "    # Iterate through each time period to extract variable values\n",
    "    for t in data.index:\n",
    "        # Extract s_power\n",
    "        s_power_var = model.getVarByName(f\"s_power[{t}]\")\n",
    "        s_power_values.append(s_power_var.X)\n",
    "        \n",
    "        # Extract SOC\n",
    "        SOC_var = model.getVarByName(f\"SOC[{t}]\")\n",
    "        SOC_values.append(SOC_var.X)\n",
    "               \n",
    "        # Extract Baseline\n",
    "        Baseline_var = model.getVarByName(f\"baseline[{t}]\")\n",
    "        Baseline_values.append(Baseline_var.X)\n",
    "        \n",
    "        # Extract F_pos\n",
    "        F_pos_var = model.getVarByName(f\"F_pos[{t}]\")\n",
    "        F_pos_values.append(F_pos_var.X)\n",
    "\n",
    "        # Calculate net consumption after optimization\n",
    "        net_consumption = data.loc[t, 'Load'] - data.loc[t, 'PV_Generation'] + s_power_values[-1]\n",
    "        NetConsumption_Optimized.append(net_consumption)\n",
    "    \n",
    "    # Add extracted values to the DataFrame\n",
    "    data['s_power'] = s_power_values\n",
    "    data['SOC'] = SOC_values\n",
    "    data['Baseline'] = Baseline_values\n",
    "    data['F_pos'] = F_pos_values\n",
    "    data['NetConsumption_Optimized'] = NetConsumption_Optimized\n",
    "\n",
    "    return data\n",
    "\n",
    "def summarize_results(data, ems_id=1):\n",
    "    \"\"\"\n",
    "    Summarize the optimization results including total costs and savings.\n",
    "    \"\"\"\n",
    "    data = data[data['ems_id'] == ems_id]\n",
    "\n",
    "    # Total Cost after optimization\n",
    "    total_cost_optimized = ((data['Price'] * data['NetConsumption_Optimized'] - data['FlexPrice'] * data['F_pos'])).sum()\n",
    "    # Total Cost without optimization (No BESS actions)\n",
    "    total_cost_unoptimized = (data['Price'] * (data['Load'] - data['PV_Generation'])).sum()\n",
    "    # Total Flexibility Revenue\n",
    "    total_flex_revenue = (data['FlexPrice'] * data['F_pos']).sum()\n",
    "    # Total Cost Savings\n",
    "    cost_savings = total_cost_unoptimized - total_cost_optimized\n",
    "    \n",
    "    # Display the summary\n",
    "    print(\"----- Optimization Summary -----\")\n",
    "    print(f\"Total Cost without Optimization: ${total_cost_unoptimized:.2f}\")\n",
    "    print(f\"Total Cost after Optimization:    ${total_cost_optimized:.2f}\")\n",
    "    print(f\"Total Flexibility Revenue:        ${total_flex_revenue:.2f}\")\n",
    "    print(f\"Total Cost Savings:               ${cost_savings:.2f}\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "def visualize_results(data, ems_id=1):\n",
    "    \"\"\"\n",
    "    Visualize the optimization results including storage power, SOC, net consumption, and flexibility.\n",
    "    \"\"\"\n",
    "    data = data[data['ems_id'] == ems_id]\n",
    "    \n",
    "    # Ensure there are at least 100 data points\n",
    "    delta=100\n",
    "    num_periods = min(delta+100, len(data))\n",
    "    data_subset = data.iloc[delta:num_periods].copy()\n",
    "\n",
    "    # Calculate Cost and Revenue per Iteration\n",
    "    data_subset['Cost'] = data_subset['Price'] * data_subset['NetConsumption_Optimized']\n",
    "    data_subset['Revenue'] = data_subset['FlexPrice'] * data_subset['F_pos']\n",
    "    # Calculate Cumulative Overall Costs\n",
    "    data_subset['Cumulative_Cost'] = data_subset['Cost'].cumsum() - data_subset['Revenue'].cumsum()\n",
    "    \n",
    "    # Identify congestion periods\n",
    "    congestion_periods = data_subset[data_subset['Congestion'] == 1]['Time']\n",
    "    \n",
    "    # Set up the plotting environment with 5 subplots using GridSpec for better layout control\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(5, 1, height_ratios=[2, 2, 2, 2, 2], hspace=0.4)\n",
    "    \n",
    "    axs = gs.subplots(sharex=True)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Plot Storage Power (s_power)\n",
    "    # ----------------------------\n",
    "    axs[0].step(data_subset['Time'], data_subset['s_power'], where='post', label='Storage Power', color='blue')\n",
    "    axs[0].step(data_subset['Time'], data_subset['SOC'], where='post', label='State of Charge', color='orange')\n",
    "    axs[0].set_ylabel('Power (kW)')\n",
    "    axs[0].set_title(f'Storage Charging/Discharging Schedule and SOC (First {num_periods} Periods)')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Highlight congestion periods\n",
    "    for ct in congestion_periods:\n",
    "        axs[0].axvspan(ct, ct, color='red', alpha=0.9)\n",
    "       \n",
    "    # ----------------------------\n",
    "    # Plot Net Consumption (NetConsumption_Optimized)\n",
    "    # ----------------------------\n",
    "    axs[1].step(data_subset['Time'], data_subset['NetConsumption_Optimized'], where='post', label='Net Consumption (Optimized)', color='green')\n",
    "    axs[1].step(data_subset['Time'], data_subset['Baseline'], where='post', label='Baseline', color='red')\n",
    "    axs[1].set_ylabel('Load (kW)')\n",
    "    axs[1].set_title(f'Net Consumption and Baseline Over Time (First {num_periods} Periods)')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Highlight congestion periods\n",
    "    for ct in congestion_periods:\n",
    "        axs[1].axvspan(ct, ct, color='red', alpha=0.9)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Plot Flexibility (Flexibility and F_pos)\n",
    "    # ----------------------------\n",
    "    #axs[2].step(data_subset['Time'], data_subset['Flexibility'], where='post', label='Flexibility', color='green')\n",
    "    axs[2].step(data_subset['Time'], data_subset['F_pos'], where='post', label='F_pos', color='red')\n",
    "    axs[2].set_ylabel('Load (kW)')\n",
    "    axs[2].set_title(f'Flexibility and F_pos Over Time (First {num_periods} Periods)')\n",
    "    axs[2].legend()\n",
    "    axs[2].grid(True)\n",
    "    \n",
    "    # Highlight congestion periods\n",
    "    for ct in congestion_periods:\n",
    "        axs[2].axvspan(ct, ct, color='red', alpha=0.9)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Plot Cost and Revenue per Iteration\n",
    "    # ----------------------------\n",
    "    axs[3].step(data_subset['Time'], data_subset['Cost'], where='post', label='Cost', color='red')\n",
    "    axs[3].step(data_subset['Time'], data_subset['Revenue'], where='post', label='Revenue', color='purple')\n",
    "    axs[3].set_ylabel('Amount ($)')\n",
    "    axs[3].set_title(f'Cost and Revenue per Iteration (First {num_periods} Periods)')\n",
    "    axs[3].legend()\n",
    "    axs[3].grid(True)\n",
    "    \n",
    "    # Highlight congestion periods\n",
    "    for ct in congestion_periods:\n",
    "        axs[3].axvspan(ct, ct, color='red', alpha=0.9)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Plot Cumulative Overall Costs\n",
    "    # ----------------------------\n",
    "    axs[4].plot(data_subset['Time'], data_subset['Cumulative_Cost'], label='Cumulative Cost', color='brown')\n",
    "    axs[4].set_ylabel('Cumulative Cost ($)')\n",
    "    axs[4].set_title(f'Cumulative Overall Costs (First {num_periods} Periods)')\n",
    "    axs[4].legend()\n",
    "    axs[4].grid(True)\n",
    "    \n",
    "    # Highlight congestion periods\n",
    "    for ct in congestion_periods:\n",
    "        axs[4].axvspan(ct, ct, color='red', alpha=0.9)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Formatting the x-axis\n",
    "    # ----------------------------\n",
    "    axs[-1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel('Time')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def solve_optimization_iterativly(data, bess_params, baseline_lookback, chunk_size_days, max_lookback):\n",
    "\n",
    "    ems_id = data['EMS_ID'].unique()\n",
    "    if len(ems_id) > 1:\n",
    "        print(\"Multiple EMS_IDs selected at once!\")\n",
    "    \n",
    "    start_date = data['Time'].min().date()\n",
    "    end_date = data['Time'].max().date()\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    current_start_date = start_date\n",
    "    while current_start_date < (end_date - pd.Timedelta(days=chunk_size_days)):\n",
    "        current_end_date = min(current_start_date + pd.Timedelta(days=chunk_size_days), end_date)\n",
    "\n",
    "        chunk_data = data[(data['Time'].dt.date >= current_start_date) & (data['Time'].dt.date < current_end_date)].copy()\n",
    "\n",
    "        baseline_indices = precompute_baseline_indices(chunk_data, baseline_lookback, max_lookback)\n",
    "        model = build_model(chunk_data, baseline_indices, bess_params)\n",
    "        model = optimize_model(model)\n",
    "\n",
    "        if model.status == GRB.OPTIMAL:\n",
    "            print(f\"Optimal solution found for {current_start_date} to {current_end_date}.\")\n",
    "            result_data = extract_results(model, chunk_data)\n",
    "            all_results.append(result_data)\n",
    "            #display(final_results)\n",
    "        elif model.status == GRB.INFEASIBLE:\n",
    "            print(f\"Model is infeasible for {current_start_date} to {current_end_date}.\")\n",
    "            model.computeIIS()\n",
    "            model.write(\"model.ilp\")\n",
    "            print(\"IIS (Irreducible Inconsistent Subsystem) written to 'model.ilp'.\")\n",
    "        else:\n",
    "            print(f\"Optimization ended with status {model.status} for {current_start_date} to {current_end_date}.\")\n",
    "\n",
    "        overlap_days = max_lookback\n",
    "        current_start_date = current_end_date - pd.Timedelta(days=overlap_days)\n",
    "\n",
    "    final_results = pd.concat(all_results, ignore_index=False)\n",
    "    final_results = final_results[~final_results.index.duplicated(keep='first')]\n",
    "    final_results['EMS_ID'] = ems_id[0]\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter lists\n",
    "cong_rebate = [1, 3, 5] # rebate in $/kWh - e.g. 5\n",
    "baseline_lookbacks = [3, 5, 10]\n",
    "num_cong_weekdays_list = [1, 2, 3]\n",
    "\n",
    "# Define possible congestion weekdays based on the number of days\n",
    "cong_weekdays_options = {\n",
    "    1: [0],          # Monday\n",
    "    2: [0, 3],       # Monday and Thursday\n",
    "    3: [0, 3, 5]     # Monday, Thursday, and Saturday\n",
    "}\n",
    "cong_hours = [10, 11, 12]  # Keeping congestion hours constant\n",
    "\n",
    "start_date = '2023-01-01'  # Start Date for data\n",
    "end_date = '2023-12-31'    # End Date for data\n",
    "num_ems = 1  # Number of different EMS Units to generate data for\n",
    "chunk_size_days = 40 # Number of days to iterativly solve the BESS MIP\n",
    "\n",
    "# Define parameters for BESS and Flexibility\n",
    "bess_params = {\n",
    "    'soc_init': 0,\n",
    "    'soc_min': 0,\n",
    "    'soc_max': 10,\n",
    "    'eta': 0.98,\n",
    "    's_max': 5,\n",
    "    'bound': 100,\n",
    "}\n",
    "\n",
    "# Loop over the different parameters\n",
    "for cong_rebate in cong_rebate:\n",
    "    for baseline_lookback in baseline_lookbacks:\n",
    "        for num_cong_weekdays in num_cong_weekdays_list:\n",
    "\n",
    "            # Set the current parameters\n",
    "            max_lookback = int(baseline_lookback*2)\n",
    "            cong_weekdays = cong_weekdays_options[num_cong_weekdays]\n",
    "\n",
    "            # Initialize an empty DataFrame to hold all results\n",
    "            all_results = pd.DataFrame()\n",
    "            for ems_id in range(1, num_ems + 1):\n",
    "\n",
    "                random_seed = ems_id * 10\n",
    "\n",
    "                # Print current configuration\n",
    "                print(f\"EMS_ID {ems_id}, Cong_Rebate {cong_rebate}, Baseline Lookback {baseline_lookback}, Cong Weekdays {cong_weekdays}\")\n",
    "\n",
    "                # CONGESTION\n",
    "                # Generate data (Load, PV, Price)\n",
    "                ems_data_cong = generate_ems_data(ems_id, start_date, end_date, cong_rebate, cong_weekdays, cong_hours, random_seed)\n",
    "                # Generate BESS charging and Net consumption Data using MIP\n",
    "                ems_results_cong = solve_optimization_iterativly(ems_data_cong, bess_params, baseline_lookback, chunk_size_days, max_lookback)\n",
    "                # Append congestion results to all_results DataFrame\n",
    "                ems_results_cong['Scenario'] = 'Cong'\n",
    "                all_results = pd.concat([all_results, ems_results_cong], ignore_index=True)\n",
    "            \n",
    "                # ----------------------------------------------------------------------\n",
    "\n",
    "                # NO CONGESTION\n",
    "                # Generate data (Load, PV, Price)\n",
    "                ems_data_no_cong = generate_ems_data(ems_id, start_date, end_date, cong_rebate, cong_weekdays=[], cong_hours=[], random_seed=random_seed)\n",
    "                # Generate BESS charging and Net consumption Data using MIP\n",
    "                ems_results_no_cong = solve_optimization_iterativly(ems_data_no_cong, bess_params, baseline_lookback, chunk_size_days, max_lookback)\n",
    "\n",
    "                # Append no congestion results to all_results DataFrame\n",
    "                ems_results_no_cong['Scenario'] = 'NoCong'\n",
    "                all_results = pd.concat([all_results, ems_results_no_cong], ignore_index=True)\n",
    "\n",
    "                # Save combined results to a single CSV file\n",
    "                cong_days_str = '_'.join(map(str, cong_weekdays))\n",
    "                cong_hours_str = '_'.join(map(str, cong_hours))\n",
    "                final_filename = f\"../../data/bess_results/Results_Rebate{cong_rebate}_Baseline{baseline_lookback}_CongDays{cong_days_str}_Hours{cong_hours_str}.csv\"\n",
    "                all_results.to_csv(final_filename, index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot results\n",
    "def compare_results(cong_data, nocong_data, start_idx, end_idx):\n",
    "    \"\"\"\n",
    "    Plot and compare results from two dataframes for a specific EMS_ID within a specified window.\n",
    "    \n",
    "    Parameters:\n",
    "    - result_cong: DataFrame with congestion results\n",
    "    - result_Nocong: DataFrame without congestion results\n",
    "    - ems_id: The EMS_ID to filter and plot\n",
    "    - start_idx: Starting index for the plot window (default 0)\n",
    "    - end_idx: Ending index for the plot window (default None, which means until the end)\n",
    "    \"\"\"   \n",
    "    ems_id = cong_data[\"EMS_ID\"][0]\n",
    "    # Convert Time to datetime if it's not already\n",
    "    cong_data['Time'] = pd.to_datetime(cong_data['Time'])\n",
    "    nocong_data['Time'] = pd.to_datetime(nocong_data['Time'])\n",
    "    \n",
    "    # Apply the window\n",
    "    cong_data = cong_data.iloc[start_idx:end_idx]\n",
    "    nocong_data = nocong_data.iloc[start_idx:end_idx]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(5, 1, figsize=(12, 8), gridspec_kw={'height_ratios': [3, 3, 3, 3, 1]}, sharex=True)\n",
    "    fig.suptitle(f'Comparison for EMS_ID: {ems_id} (Index {start_idx} to {end_idx if end_idx else \"end\"})')\n",
    "\n",
    "    # Define colors for the lines\n",
    "    color_with_cong = '#1f77b4'  # Blue\n",
    "    color_without_cong = '#ff7f0e'  # Orange\n",
    "\n",
    "    # Highlight congestion intervals and potential congestion hours\n",
    "    for ax in axs:\n",
    "        for _, row in cong_data.iterrows():\n",
    "            time = row['Time']\n",
    "            if row['Congestion'] == 1:\n",
    "                # Red background for congestion days\n",
    "                ax.axvspan(time, time + pd.Timedelta(hours=1), facecolor='red', alpha=0.2)\n",
    "            else:\n",
    "                # Grey background for potential congestion hours (10, 11, 12) on non-congestion days\n",
    "                if time.hour in [10, 11, 12]:\n",
    "                    ax.axvspan(time, time + pd.Timedelta(hours=1), facecolor='grey', alpha=0.2)\n",
    "        \n",
    "        # Add dotted lines for day endings\n",
    "        day_ends = cong_data[cong_data['Time'].dt.hour == 23]['Time']\n",
    "        for day_end in day_ends:\n",
    "            ax.axvline(day_end, color='black', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    # Plot Load\n",
    "    axs[0].plot(nocong_data['Time'], nocong_data['Load'], label='Without Congestion', color=color_without_cong)\n",
    "    axs[0].plot(cong_data['Time'], cong_data['Load'], label='With Congestion', color=color_with_cong)\n",
    "    axs[0].set_ylabel('Load')\n",
    "    axs[0].legend(loc='upper right')\n",
    "    \n",
    "    # Plot PV Generation\n",
    "    axs[1].plot(nocong_data['Time'], nocong_data['PV_Generation'], label='Without Congestion', color=color_without_cong)\n",
    "    axs[1].plot(cong_data['Time'], cong_data['PV_Generation'], label='With Congestion', color=color_with_cong)\n",
    "    axs[1].set_ylabel('PV Generation')\n",
    "    axs[1].legend(loc='upper right')\n",
    "    \n",
    "    # Plot Price\n",
    "    axs[2].plot(nocong_data['Time'], nocong_data['Price'], label='Without Congestion', color=color_without_cong)\n",
    "    axs[2].plot(cong_data['Time'], cong_data['Price'], label='With Congestion', color=color_with_cong)\n",
    "    axs[2].set_ylabel('Price')\n",
    "    axs[2].legend(loc='upper right')\n",
    "    \n",
    "    # Plot Net Consumption\n",
    "    axs[3].plot(nocong_data['Time'], nocong_data['NetConsumption_Optimized'], label='Without Congestion', color=color_without_cong)\n",
    "    axs[3].plot(cong_data['Time'], cong_data['NetConsumption_Optimized'], label='With Congestion', color=color_with_cong)\n",
    "    axs[3].set_ylabel('Net Consumption')\n",
    "    axs[3].legend(loc='upper right')\n",
    "    \n",
    "    # Plot Net Consumption Difference in a separate small subplot\n",
    "    diff = cong_data['NetConsumption_Optimized'] - nocong_data['NetConsumption_Optimized']\n",
    "    bar_width = pd.Timedelta(hours=1)\n",
    "\n",
    "    congestion_color = 'red'\n",
    "    non_congestion_color = 'green'\n",
    "    bar_colors = [congestion_color if time.hour in [10, 11, 12] else non_congestion_color for time in cong_data['Time']]\n",
    "    \n",
    "    axs[4].bar(cong_data['Time'], diff, width=bar_width.total_seconds() / 3600 / 24, alpha=0.8, color=bar_colors)\n",
    "    axs[4].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    axs[4].set_ylabel('Difference')\n",
    "\n",
    "    # Add a legend for the bar colors\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=congestion_color, edgecolor='black', label='Congestion Hours'),\n",
    "                       Patch(facecolor=non_congestion_color, edgecolor='black', label='Non-Congestion Hours')]\n",
    "    axs[4].legend(handles=legend_elements, loc='upper right', fontsize='small')\n",
    "    \n",
    "    # Set x-axis label for the bottom subplot\n",
    "    axs[4].set_xlabel('Time')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.setp(axs[3].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Adjust layout and display plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#result = pd.read_csv(\"../../data/bess_results/Results_Rebate1_Baseline3_CongDays0_3_5_Hours10_11_12.csv\")\n",
    "#result_cong = result[result[\"Scenario\"]==\"Cong\"].reset_index(drop=True)\n",
    "#result_nocong = result[result[\"Scenario\"]==\"NoCong\"].reset_index(drop=True)\n",
    "\n",
    "#compare_results(result_cong, result_nocong, start_idx=100, end_idx=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
